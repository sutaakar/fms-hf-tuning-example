{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ce1e7c-1ba1-4712-aa3a-b5f2bde689d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting s3fs\n",
      "  Downloading s3fs-2025.3.2-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting aiobotocore<3.0.0,>=2.5.4 (from s3fs)\n",
      "  Downloading aiobotocore-2.22.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: fsspec==2025.3.2.* in /opt/app-root/lib64/python3.11/site-packages (from s3fs) (2025.3.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/app-root/lib64/python3.11/site-packages (from s3fs) (3.11.16)\n",
      "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs)\n",
      "  Downloading aioitertools-0.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting botocore<1.37.4,>=1.37.2 (from aiobotocore<3.0.0,>=2.5.4->s3fs)\n",
      "  Downloading botocore-1.37.3-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/app-root/lib64/python3.11/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (2.9.0.post0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/app-root/lib64/python3.11/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.0.1)\n",
      "Requirement already satisfied: multidict<7.0.0,>=6.0.0 in /opt/app-root/lib64/python3.11/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (6.4.3)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /opt/app-root/lib64/python3.11/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.17.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.19.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/app-root/lib64/python3.11/site-packages (from botocore<1.37.4,>=1.37.2->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.26.20)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib64/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.17.0)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/app-root/lib64/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.10)\n",
      "Downloading s3fs-2025.3.2-py3-none-any.whl (30 kB)\n",
      "Downloading aiobotocore-2.22.0-py3-none-any.whl (78 kB)\n",
      "Downloading aioitertools-0.12.0-py3-none-any.whl (24 kB)\n",
      "Downloading botocore-1.37.3-py3-none-any.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m257.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: aioitertools, botocore, aiobotocore, s3fs\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.37.34\n",
      "    Uninstalling botocore-1.37.34:\n",
      "      Successfully uninstalled botocore-1.37.34\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "boto3 1.37.34 requires botocore<1.38.0,>=1.37.34, but you have botocore 1.37.3 which is incompatible.\n",
      "s3transfer 0.11.4 requires botocore<2.0a.0,>=1.37.4, but you have botocore 1.37.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiobotocore-2.22.0 aioitertools-0.12.0 botocore-1.37.3 s3fs-2025.3.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5054cfef-db4c-4a2b-8139-9f704da238b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_trainer_config = '''{\n",
    "    \"model_name_or_path\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"training_data_path\": \"thesven/gsm8k-reasoning\",\n",
    "    \"output_dir\": \"/mnt/output/model\",\n",
    "    \"save_model_dir\": \"/mnt/output/model\",\n",
    "    \"num_train_epochs\": 1.0,\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"per_device_eval_batch_size\": 4,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"gradient_checkpointing\": true,\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"include_tokens_per_second\": true,\n",
    "    \"data_formatter_template\": \"### Question:{{question}} \\\\n\\\\n### Answer: {{answer}}\",\n",
    "    \"response_template\": \"\\\\n### Answer:\",\n",
    "    \"use_flash_attn\": true,\n",
    "    \"fast_kernels\": [true, true, true],\n",
    "    \"peft_method\": \"lora\",\n",
    "    \"lora_post_process_for_vllm\": true\n",
    "}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0842c2f-7e9b-4756-b29a-563413a0243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def encode_config(config):\n",
    "    base64_bytes = base64.b64encode(config.encode(\"ascii\"))\n",
    "    txt = base64_bytes.decode(\"ascii\")\n",
    "    return txt\n",
    "\n",
    "encoded_config = encode_config(sft_trainer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f03403d-3402-43db-881d-5e9864a67f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.training import TrainingClient\n",
    "from kubernetes import client\n",
    "from kubernetes.client import (\n",
    "    V1EnvVar,\n",
    "    V1EnvVarSource,\n",
    "    V1SecretKeySelector,\n",
    "    V1Volume,\n",
    "    V1VolumeMount,\n",
    "    V1PersistentVolumeClaimVolumeSource\n",
    ")\n",
    "\n",
    "job_name = \"llama4\"\n",
    "\n",
    "tc = TrainingClient()\n",
    "\n",
    "tc.create_job(\n",
    "    job_kind=\"PyTorchJob\",\n",
    "    name=job_name,\n",
    "    num_workers=1,\n",
    "    num_procs_per_worker=\"auto\",\n",
    "    resources_per_worker={\"gpu\": 1},\n",
    "    base_image=\"quay.io/modh/fms-hf-tuning:v2.8.2\",\n",
    "    env_vars=[\n",
    "        V1EnvVar(name=\"HF_TOKEN\", value_from=V1EnvVarSource(secret_key_ref=V1SecretKeySelector(key=\"HF_TOKEN\", name=\"hf-token\"))),\n",
    "        V1EnvVar(name=\"SFT_TRAINER_CONFIG_JSON_ENV_VAR\", value=encoded_config),\n",
    "    ],\n",
    "    volumes=[\n",
    "        V1Volume(name=\"llama70\",\n",
    "                 persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(claim_name=\"llama70\")),\n",
    "        V1Volume(name=\"trained-model\",\n",
    "                 persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(claim_name=\"trained-model\")),\n",
    "    ],\n",
    "    volume_mounts=[\n",
    "        V1VolumeMount(name=\"llama70\", mount_path=\"/mnt/model\"),\n",
    "        V1VolumeMount(name=\"trained-model\", mount_path=\"/mnt/output\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb2239c9-3bc9-436a-9c04-e9372e153a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pod llama4-master-0]: WARNING:accelerate.commands.launch:The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "[Pod llama4-master-0]: \t`--num_machines` was set to a value of `1`\n",
      "[Pod llama4-master-0]: \t`--mixed_precision` was set to a value of `'no'`\n",
      "[Pod llama4-master-0]: \t`--dynamo_backend` was set to a value of `'no'`\n",
      "[Pod llama4-master-0]: To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "[Pod llama4-master-0]: /home/tuning/.local/lib/python3.12/site-packages/fms_acceleration_moe/utils/checkpoint_utils.py:367: SyntaxWarning: invalid escape sequence '\\.'\n",
      "[Pod llama4-master-0]:   _reg = re.compile(f\"(.*)\\.({_name})\\.weight\")\n",
      "[Pod llama4-master-0]: /home/tuning/.local/lib/python3.12/site-packages/fms_acceleration_moe/utils/checkpoint_utils.py:367: SyntaxWarning: invalid escape sequence '\\.'\n",
      "[Pod llama4-master-0]:   _reg = re.compile(f\"(.*)\\.({_name})\\.weight\")\n",
      "[Pod llama4-master-0]: /home/tuning/.local/lib/python3.12/site-packages/fms_acceleration_moe/utils/scattermoe_state_dict.py:159: SyntaxWarning: invalid escape sequence '\\.'\n",
      "[Pod llama4-master-0]:   m = re.match(f\"({router_name}|{expert_name})\\.?(\\d+)?\\.?(\\w+)?\\.weight\", rel_k)\n",
      "Fetching 4 files: 100%|██████████| 4/4 [00:16<00:00,  4.17s/it]\n",
      "[Pod llama4-master-0]: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.19it/s]\n",
      "[Pod llama4-master-0]: WARNING:tokenizer_data_utils.py:PAD token set to default, missing in tokenizer\n",
      "[Pod llama4-master-0]: WARNING:tokenizer_data_utils.py:UNK token set to default, missing in tokenizer\n",
      "[Pod llama4-master-0]: The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "[Pod llama4-master-0]: The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "Generating train split: 100%|██████████| 6914/6914 [00:00<00:00, 132602.11 examples/s]\n",
      "Map (num_proc=80): 100%|██████████| 6914/6914 [00:11<00:00, 596.75 examples/s]\n",
      "Converting train dataset to ChatML: 100%|██████████| 6914/6914 [00:00<00:00, 7621.10 examples/s]\n",
      "Applying chat template to train dataset: 100%|██████████| 6914/6914 [00:01<00:00, 5970.09 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 6914/6914 [00:02<00:00, 3083.56 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 6914/6914 [00:00<00:00, 294616.71 examples/s]\n",
      "[Pod llama4-master-0]: No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "  0%|          | 0/1728 [00:00<?, ?it/s]/home/tuning/.local/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "[Pod llama4-master-0]:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "[Pod llama4-master-0]: /home/tuning/.local/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "[Pod llama4-master-0]:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "  0%|          | 1/1728 [00:02<1:24:00,  2.92s/it]/home/tuning/.local/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "[Pod llama4-master-0]:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "[Pod llama4-master-0]: /home/tuning/.local/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "[Pod llama4-master-0]:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.8281, 'grad_norm': 6.440357208251953, 'learning_rate': 0.0, 'num_tokens': 638481.0, 'mean_token_accuracy': 0.77422891868951, 'epoch': 1.0}\n",
      "{'train_runtime': 1061.6937, 'train_samples_per_second': 6.512, 'train_steps_per_second': 1.628, 'train_tokens_per_second': 601.545, 'train_loss': 0.8280536510326244, 'epoch': 1.0}\n",
      "100%|██████████| 1728/1728 [17:41<00:00,  1.63it/s]\n",
      "[Pod llama4-master-0]: /home/tuning/.local/lib/python3.12/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "[Pod llama4-master-0]:   warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "logs, _ = tc.get_job_logs(job_name, follow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1de38326-1e2a-4fe2-85d0-59c8f582546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import os\n",
    "\n",
    "\n",
    "s3 = s3fs.S3FileSystem(\n",
    "      key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "      secret=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "      endpoint_url=os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "   )\n",
    "s3_path = os.environ[\"AWS_S3_BUCKET\"] + \"/meta-llama/Llama-3.1-8B-Instruct/adapter\"\n",
    "_ = s3.put(\"/opt/app-root/src/trained-model-1/model\", s3_path, recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5db986b-a117-47a4-9de0-bd3cc3d1bac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.78.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/app-root/lib64/python3.11/site-packages (from openai) (4.9.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/app-root/lib64/python3.11/site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/app-root/lib64/python3.11/site-packages (from openai) (1.10.21)\n",
      "Requirement already satisfied: sniffio in /opt/app-root/lib64/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/app-root/lib64/python3.11/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/app-root/lib64/python3.11/site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/app-root/lib64/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/app-root/lib64/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/app-root/lib64/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/app-root/lib64/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Downloading openai-1.78.0-py3-none-any.whl (680 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m680.4/680.4 kB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (351 kB)\n",
      "Installing collected packages: jiter, distro, openai\n",
      "Successfully installed distro-1.9.0 jiter-0.9.0 openai-1.78.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13e1bc97-545e-4068-955f-d34531a1ec72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find out how many clips Natalia sold in May, we need to calculate half of the clips she sold in April. \n",
      "\n",
      "In April, Natalia sold 48 clips. \n",
      "Half of 48 is 48 / 2 = 24 clips.\n",
      "\n",
      "So, in May, Natalia sold 24 clips. \n",
      "\n",
      "To find the total number of clips sold in April and May, we need to add the clips sold in both months. \n",
      "\n",
      "48 (April) + 24 (May) = 72 clips.\n",
      "\n",
      "Natalia sold 72 clips altogether in April and May.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import httpx\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    http_client = httpx.Client(verify=False) \n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"llama4\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7c98eb-8250-4380-9d91-0b6a32b75810",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
