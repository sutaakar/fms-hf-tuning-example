{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ce1e7c-1ba1-4712-aa3a-b5f2bde689d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "boto3 1.37.34 requires botocore<1.38.0,>=1.37.34, but you have botocore 1.37.3 which is incompatible.\n",
      "s3transfer 0.11.4 requires botocore<2.0a.0,>=1.37.4, but you have botocore 1.37.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qqU s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5054cfef-db4c-4a2b-8139-9f704da238b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_trainer_config = '''{\n",
    "    \"accelerate_launch_args\": {\n",
    "        \"use_fsdp\": true,\n",
    "        \"fsdp_version\": 2,\n",
    "        \"fsdp_auto_wrap_policy\": \"TRANSFORMER_BASED_WRAP\",\n",
    "        \"fsdp_sharding_strategy\": \"FULL_SHARD\",\n",
    "        \"fsdp_state_dict_type\": \"SHARDED_STATE_DICT\",\n",
    "        \"fsdp_cpu_ram_efficient_loading\": true\n",
    "      },\n",
    "    \"model_name_or_path\": \"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n",
    "    \"training_data_path\": \"epfl-dlab/gsm8k\",\n",
    "    \"validation_data_path\": \"epfl-dlab/gsm8k\",\n",
    "    \"output_dir\": \"/mnt/output/model\",\n",
    "    \"save_model_dir\": \"/mnt/output/model\",\n",
    "    \"num_train_epochs\": 3.0,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"per_device_eval_batch_size\": 4,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"gradient_checkpointing\": true,\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"include_tokens_per_second\": true,\n",
    "    \"data_formatter_template\": \"### Question:{{question}} \\\\n\\\\n### Answer: {{answer}}\",\n",
    "    \"response_template\": \"\\\\n### Answer:\",\n",
    "    \"use_flash_attn\": false,\n",
    "    \"fast_moe\": true,\n",
    "    \"torch_dtype\": \"bfloat16\",\n",
    "    \"max_seq_length\": 4096,\n",
    "    \"peft_method\": \"lora\",\n",
    "    \"r\": 8,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    \"lora_post_process_for_vllm\": true\n",
    "}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d0842c2f-7e9b-4756-b29a-563413a0243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training configuration to base64 representation to pass it using environment variable\n",
    "import base64\n",
    "\n",
    "\n",
    "def encode_config(config):\n",
    "    base64_bytes = base64.b64encode(config.encode(\"ascii\"))\n",
    "    txt = base64_bytes.decode(\"ascii\")\n",
    "    return txt\n",
    "\n",
    "encoded_config = encode_config(sft_trainer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f03403d-3402-43db-881d-5e9864a67f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.training import TrainingClient\n",
    "from kubernetes.client import (\n",
    "    V1EnvVar,\n",
    "    V1EnvVarSource,\n",
    "    V1SecretKeySelector,\n",
    "    V1Volume,\n",
    "    V1VolumeMount,\n",
    "    V1PersistentVolumeClaimVolumeSource\n",
    ")\n",
    "\n",
    "job_name = \"llama4\"\n",
    "\n",
    "tc = TrainingClient()\n",
    "\n",
    "tc.create_job(\n",
    "    job_kind=\"PyTorchJob\",\n",
    "    name=job_name,\n",
    "    num_workers=1,\n",
    "    num_procs_per_worker=\"auto\",\n",
    "    resources_per_worker={\"gpu\": 8},\n",
    "    base_image=\"image-registry.openshift-image-registry.svc:5000/fms-hf-tuning/fms-hf-tuning:2.8.2\",\n",
    "    env_vars=[\n",
    "        V1EnvVar(name=\"HF_TOKEN\", value_from=V1EnvVarSource(secret_key_ref=V1SecretKeySelector(key=\"HF_TOKEN\", name=\"hf-token\"))),\n",
    "        V1EnvVar(name=\"HF_HOME\", value=\"/mnt/scratch/hf_home\"),\n",
    "        V1EnvVar(name=\"SFT_TRAINER_CONFIG_JSON_ENV_VAR\", value=encoded_config),\n",
    "    ],\n",
    "    volumes=[\n",
    "        V1Volume(name=\"trained-model\",\n",
    "                 persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(claim_name=\"trained-model\")),\n",
    "        V1Volume(name=\"scratch-volume\",\n",
    "                 persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(claim_name=\"scratch-volume\")),\n",
    "    ],\n",
    "    volume_mounts=[\n",
    "        V1VolumeMount(name=\"trained-model\", mount_path=\"/mnt/output\"),\n",
    "        V1VolumeMount(name=\"scratch-volume\", mount_path=\"/mnt/scratch\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2239c9-3bc9-436a-9c04-e9372e153a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pod llama4-master-0]: WARNING:accelerate.commands.launch:The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "[Pod llama4-master-0]: \t`--num_machines` was set to a value of `1`\n",
      "[Pod llama4-master-0]: \t`--mixed_precision` was set to a value of `'no'`\n",
      "[Pod llama4-master-0]: \t`--dynamo_backend` was set to a value of `'no'`\n",
      "[Pod llama4-master-0]: To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "[Pod llama4-master-0]: WARNING:sft_trainer.py:You are running lora with the ScatterMoE plugin, please note that passing target modules that are part of the moe module can cause unexpected behaviors and unsuccessful tuning while LoRA tuning with ScatterMoE. For safe tuning, only pass linear modules such as those in the attn layer (i.e. ['q_proj', 'v_proj', 'o_proj', 'k_proj'])\n",
      "Fetching 50 files:   0%|          | 0/50 [00:00<?, ?it/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: WARNING:file_download.py:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: WARNING:file_download.py:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: WARNING:file_download.py:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: WARNING:file_download.py:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: WARNING:file_download.py:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: WARNING:file_download.py:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: WARNING:file_download.py:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: WARNING:file_download.py:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: WARNING:file_download.py:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: WARNING:file_download.py:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: WARNING:file_download.py:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: WARNING:file_download.py:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: WARNING:file_download.py:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: WARNING:file_download.py:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: WARNING:file_download.py:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: WARNING:file_download.py:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: WARNING:file_download.py:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: WARNING:file_download.py:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: WARNING:file_download.py:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: WARNING:file_download.py:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: WARNING:file_download.py:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: /home/tuning/.local/lib/python3.12/site-packages/huggingface_hub/file_download.py:752: UserWarning: Not enough free disk space to download the file. The expected file size is: 4404.21 MB. The target location /.cache/huggingface/hub/models--meta-llama--Llama-4-Scout-17B-16E-Instruct/blobs only has 4268.51 MB free disk space.\n",
      "[Pod llama4-master-0]:   warnings.warn(\n",
      "[Pod llama4-master-0]: Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: WARNING:file_download.py:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: /home/tuning/.local/lib/python3.12/site-packages/huggingface_hub/file_download.py:752: UserWarning: Not enough free disk space to download the file. The expected file size is: 4404.21 MB. The target location /.cache/huggingface/hub/models--meta-llama--Llama-4-Scout-17B-16E-Instruct/blobs only has 3701.49 MB free disk space.\n",
      "[Pod llama4-master-0]:   warnings.warn(\n",
      "[Pod llama4-master-0]: Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: WARNING:file_download.py:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: /home/tuning/.local/lib/python3.12/site-packages/huggingface_hub/file_download.py:752: UserWarning: Not enough free disk space to download the file. The expected file size is: 4404.21 MB. The target location /.cache/huggingface/hub/models--meta-llama--Llama-4-Scout-17B-16E-Instruct/blobs only has 3680.31 MB free disk space.\n",
      "[Pod llama4-master-0]:   warnings.warn(\n",
      "[Pod llama4-master-0]: Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: WARNING:file_download.py:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Fetching 50 files:   2%|▏         | 1/50 [01:35<1:18:07, 95.67s/it]/home/tuning/.local/lib/python3.12/site-packages/huggingface_hub/file_download.py:752: UserWarning: Not enough free disk space to download the file. The expected file size is: 4404.21 MB. The target location /.cache/huggingface/hub/models--meta-llama--Llama-4-Scout-17B-16E-Instruct/blobs only has 1102.51 MB free disk space.\n",
      "[Pod llama4-master-0]:   warnings.warn(\n",
      "[Pod llama4-master-0]: Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: WARNING:file_download.py:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: /home/tuning/.local/lib/python3.12/site-packages/huggingface_hub/file_download.py:752: UserWarning: Not enough free disk space to download the file. The expected file size is: 4404.21 MB. The target location /.cache/huggingface/hub/models--meta-llama--Llama-4-Scout-17B-16E-Instruct/blobs only has 1103.82 MB free disk space.\n",
      "[Pod llama4-master-0]:   warnings.warn(\n",
      "[Pod llama4-master-0]: Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: WARNING:file_download.py:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Fetching 50 files:   4%|▍         | 2/50 [01:42<34:34, 43.23s/it]  /home/tuning/.local/lib/python3.12/site-packages/huggingface_hub/file_download.py:752: UserWarning: Not enough free disk space to download the file. The expected file size is: 4404.21 MB. The target location /.cache/huggingface/hub/models--meta-llama--Llama-4-Scout-17B-16E-Instruct/blobs only has 497.41 MB free disk space.\n",
      "[Pod llama4-master-0]:   warnings.warn(\n",
      "[Pod llama4-master-0]: Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "[Pod llama4-master-0]: WARNING:file_download.py:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "logs, _ = tc.get_job_logs(job_name, follow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1de38326-1e2a-4fe2-85d0-59c8f582546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload trained LoRA layer to the S3 bucket containing base model\n",
    "import s3fs\n",
    "import os\n",
    "\n",
    "\n",
    "s3 = s3fs.S3FileSystem(\n",
    "      key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "      secret=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "      endpoint_url=os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "   )\n",
    "s3_path = os.environ[\"AWS_S3_BUCKET\"] + \"/meta-llama/Llama-4-Scout-17B-16E-Instruct/adapter\"\n",
    "_ = s3.put(\"/opt/app-root/src/model\", s3_path, recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d018ec0-c057-4335-9417-8d348d397047",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
