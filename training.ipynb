{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "56ce1e7c-1ba1-4712-aa3a-b5f2bde689d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qqU s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5054cfef-db4c-4a2b-8139-9f704da238b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_trainer_config = '''{\n",
    "    \"model_name_or_path\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"training_data_path\": \"epfl-dlab/gsm8k\",\n",
    "    \"validation_data_path\": \"epfl-dlab/gsm8k\",\n",
    "    \"output_dir\": \"/mnt/output/model\",\n",
    "    \"save_model_dir\": \"/mnt/output/model\",\n",
    "    \"num_train_epochs\": 7.0,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"per_device_eval_batch_size\": 4,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"gradient_checkpointing\": true,\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"include_tokens_per_second\": true,\n",
    "    \"data_formatter_template\": \"### Question:{{question}} \\\\n\\\\n### Answer: {{answer}}\",\n",
    "    \"response_template\": \"\\\\n### Answer:\",\n",
    "    \"use_flash_attn\": true,\n",
    "    \"fast_kernels\": [true, true, true],\n",
    "    \"peft_method\": \"lora\",\n",
    "    \"r\": 8,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    \"lora_post_process_for_vllm\": true\n",
    "}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d0842c2f-7e9b-4756-b29a-563413a0243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training configuration to base64 representation to pass it using environment variable\n",
    "import base64\n",
    "\n",
    "\n",
    "def encode_config(config):\n",
    "    base64_bytes = base64.b64encode(config.encode(\"ascii\"))\n",
    "    txt = base64_bytes.decode(\"ascii\")\n",
    "    return txt\n",
    "\n",
    "encoded_config = encode_config(sft_trainer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2f03403d-3402-43db-881d-5e9864a67f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.training import TrainingClient\n",
    "from kubernetes.client import (\n",
    "    V1EnvVar,\n",
    "    V1EnvVarSource,\n",
    "    V1SecretKeySelector,\n",
    "    V1Volume,\n",
    "    V1VolumeMount,\n",
    "    V1PersistentVolumeClaimVolumeSource\n",
    ")\n",
    "\n",
    "job_name = \"llama4\"\n",
    "\n",
    "tc = TrainingClient()\n",
    "\n",
    "tc.create_job(\n",
    "    job_kind=\"PyTorchJob\",\n",
    "    name=job_name,\n",
    "    num_workers=1,\n",
    "    num_procs_per_worker=\"auto\",\n",
    "    resources_per_worker={\"gpu\": 1},\n",
    "    base_image=\"quay.io/modh/fms-hf-tuning:v2.8.2\",\n",
    "    env_vars=[\n",
    "        V1EnvVar(name=\"HF_TOKEN\", value_from=V1EnvVarSource(secret_key_ref=V1SecretKeySelector(key=\"HF_TOKEN\", name=\"hf-token\"))),\n",
    "        V1EnvVar(name=\"SFT_TRAINER_CONFIG_JSON_ENV_VAR\", value=encoded_config),\n",
    "    ],\n",
    "    volumes=[\n",
    "        V1Volume(name=\"trained-model\",\n",
    "                 persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(claim_name=\"trained-model\")),\n",
    "    ],\n",
    "    volume_mounts=[\n",
    "        V1VolumeMount(name=\"trained-model\", mount_path=\"/mnt/output\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cb2239c9-3bc9-436a-9c04-e9372e153a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pod llama4-master-0]: WARNING:accelerate.commands.launch:The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "[Pod llama4-master-0]: \t`--num_machines` was set to a value of `1`\n",
      "[Pod llama4-master-0]: \t`--mixed_precision` was set to a value of `'no'`\n",
      "[Pod llama4-master-0]: \t`--dynamo_backend` was set to a value of `'no'`\n",
      "[Pod llama4-master-0]: To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "[Pod llama4-master-0]: /home/tuning/.local/lib/python3.12/site-packages/fms_acceleration_moe/utils/checkpoint_utils.py:367: SyntaxWarning: invalid escape sequence '\\.'\n",
      "[Pod llama4-master-0]:   _reg = re.compile(f\"(.*)\\.({_name})\\.weight\")\n",
      "[Pod llama4-master-0]: /home/tuning/.local/lib/python3.12/site-packages/fms_acceleration_moe/utils/checkpoint_utils.py:367: SyntaxWarning: invalid escape sequence '\\.'\n",
      "[Pod llama4-master-0]:   _reg = re.compile(f\"(.*)\\.({_name})\\.weight\")\n",
      "[Pod llama4-master-0]: /home/tuning/.local/lib/python3.12/site-packages/fms_acceleration_moe/utils/scattermoe_state_dict.py:159: SyntaxWarning: invalid escape sequence '\\.'\n",
      "[Pod llama4-master-0]:   m = re.match(f\"({router_name}|{expert_name})\\.?(\\d+)?\\.?(\\w+)?\\.weight\", rel_k)\n",
      "Fetching 4 files: 100%|██████████| 4/4 [06:02<00:00, 90.69s/it] \n",
      "[Pod llama4-master-0]: You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.21it/s]\n",
      "[Pod llama4-master-0]: WARNING:tokenizer_data_utils.py:PAD token set to default, missing in tokenizer\n",
      "[Pod llama4-master-0]: WARNING:tokenizer_data_utils.py:UNK token set to default, missing in tokenizer\n",
      "[Pod llama4-master-0]: The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "[Pod llama4-master-0]: The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "Generating train split: 100%|██████████| 7473/7473 [00:00<00:00, 492908.22 examples/s]\n",
      "Generating test split: 100%|██████████| 1319/1319 [00:00<00:00, 331850.94 examples/s]\n",
      "Map (num_proc=80): 100%|██████████| 7473/7473 [00:11<00:00, 660.20 examples/s]\n",
      "Converting train dataset to ChatML: 100%|██████████| 7473/7473 [00:00<00:00, 26015.77 examples/s]\n",
      "Adding EOS to train dataset: 100%|██████████| 7473/7473 [00:00<00:00, 30229.76 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 7473/7473 [00:02<00:00, 2552.27 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 7473/7473 [00:00<00:00, 592268.50 examples/s]\n",
      "[Pod llama4-master-0]: No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "  0%|          | 0/3269 [00:00<?, ?it/s]/home/tuning/.local/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "[Pod llama4-master-0]:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "[Pod llama4-master-0]: /home/tuning/.local/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "[Pod llama4-master-0]:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "  0%|          | 4/3269 [00:09<1:49:04,  2.00s/it]/home/tuning/.local/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "[Pod llama4-master-0]:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "[Pod llama4-master-0]: /home/tuning/.local/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "[Pod llama4-master-0]:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      " 14%|█▍        | 467/3269 [13:40<1:26:55,  1.86s/it]/home/tuning/.local/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "[Pod llama4-master-0]:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.4532, 'grad_norm': 4.8450846672058105, 'learning_rate': 9.502757392681414e-06, 'num_tokens': 1242619.0, 'mean_token_accuracy': 0.8676002105970061, 'epoch': 1.0}\n",
      " 14%|█▍        | 468/3269 [13:41<1:08:44,  1.47s/it]/home/tuning/.local/lib/python3.12/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "[Pod llama4-master-0]:   warnings.warn(\n",
      "[Pod llama4-master-0]: /home/tuning/.local/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "[Pod llama4-master-0]:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.4133, 'grad_norm': 2.561213731765747, 'learning_rate': 8.109929654938844e-06, 'num_tokens': 2485238.0, 'mean_token_accuracy': 0.8770634599584127, 'epoch': 2.0}\n",
      " 29%|██▊       | 936/3269 [27:20<47:48,  1.23s/it]/home/tuning/.local/lib/python3.12/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "[Pod llama4-master-0]:   warnings.warn(\n",
      "[Pod llama4-master-0]: /home/tuning/.local/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "[Pod llama4-master-0]:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.391, 'grad_norm': 6.795107841491699, 'learning_rate': 6.0985461051166025e-06, 'num_tokens': 3727857.0, 'mean_token_accuracy': 0.882332712906194, 'epoch': 3.0}\n",
      " 43%|████▎     | 1404/3269 [41:04<40:19,  1.30s/it]/home/tuning/.local/lib/python3.12/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "[Pod llama4-master-0]:   warnings.warn(\n",
      "[Pod llama4-master-0]: /home/tuning/.local/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "[Pod llama4-master-0]:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.3686, 'grad_norm': 9.546045303344727, 'learning_rate': 3.868664983467215e-06, 'num_tokens': 4970476.0, 'mean_token_accuracy': 0.8879575213051659, 'epoch': 4.0}\n",
      " 57%|█████▋    | 1872/3269 [54:43<28:35,  1.23s/it]/home/tuning/.local/lib/python3.12/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "[Pod llama4-master-0]:   warnings.warn(\n",
      "[Pod llama4-master-0]: /home/tuning/.local/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "[Pod llama4-master-0]:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.3518, 'grad_norm': 7.325347900390625, 'learning_rate': 1.8638030511664612e-06, 'num_tokens': 6213095.0, 'mean_token_accuracy': 0.8924454834258384, 'epoch': 5.0}\n",
      " 72%|███████▏  | 2340/3269 [1:08:24<19:16,  1.24s/it]/home/tuning/.local/lib/python3.12/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "[Pod llama4-master-0]:   warnings.warn(\n",
      "[Pod llama4-master-0]: /home/tuning/.local/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "[Pod llama4-master-0]:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.3391, 'grad_norm': 14.096356391906738, 'learning_rate': 4.827214180267431e-07, 'num_tokens': 7455714.0, 'mean_token_accuracy': 0.8960120285048467, 'epoch': 6.0}\n",
      " 86%|████████▌ | 2808/3269 [1:22:06<09:59,  1.30s/it]/home/tuning/.local/lib/python3.12/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "[Pod llama4-master-0]:   warnings.warn(\n",
      "[Pod llama4-master-0]: /home/tuning/.local/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "[Pod llama4-master-0]:   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.3329, 'grad_norm': 2.378377676010132, 'learning_rate': 0.0, 'num_tokens': 8682057.0, 'mean_token_accuracy': 0.8974295667738305, 'epoch': 6.99}\n",
      "100%|██████████| 3269/3269 [1:35:41<00:00,  1.78s/it]/home/tuning/.local/lib/python3.12/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "[Pod llama4-master-0]:   warnings.warn(\n",
      "[Pod llama4-master-0]: {'train_runtime': 5745.4037, 'train_samples_per_second': 9.105, 'train_steps_per_second': 0.569, 'train_tokens_per_second': 2097.195, 'train_loss': 0.3786455342878293, 'epoch': 6.99}\n",
      "100%|██████████| 3269/3269 [1:35:45<00:00,  1.76s/it]\n",
      "[Pod llama4-master-0]: /home/tuning/.local/lib/python3.12/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "[Pod llama4-master-0]:   warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "logs, _ = tc.get_job_logs(job_name, follow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1de38326-1e2a-4fe2-85d0-59c8f582546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload trained LoRA layer to the S3 bucket containing base model\n",
    "import s3fs\n",
    "import os\n",
    "\n",
    "\n",
    "s3 = s3fs.S3FileSystem(\n",
    "      key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "      secret=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "      endpoint_url=os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "   )\n",
    "s3_path = os.environ[\"AWS_S3_BUCKET\"] + \"/meta-llama/Llama-3.1-8B-Instruct/adapter\"\n",
    "_ = s3.put(\"/opt/app-root/src/model\", s3_path, recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d018ec0-c057-4335-9417-8d348d397047",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
